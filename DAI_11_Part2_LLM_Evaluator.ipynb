{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d08951",
   "metadata": {},
   "source": [
    "# Assignment 11 â€“ Part 2: LLM Q&A Evaluator (OpenAI)\n",
    "\n",
    "In this notebook I build a Q&A evaluator using an LLM from OpenAI.  \n",
    "The goal is to:\n",
    "\n",
    "- Load a dataset of ML questions and reference answers.\n",
    "- Let a student answer a question.\n",
    "- Use an LLM to compare the student answer with the reference.\n",
    "- Produce qualitative feedback and a numerical score (0â€“100).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88519ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet openai\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5fc7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš ï¸ Set your OpenAI API key here or via Colab's environment settings.\n",
    "# Option A (simple but less secure): uncomment and paste your key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-REPLACE_WITH_YOUR_KEY\"\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "assert client.api_key not in (None, \"\", \"sk-REPLACE_WITH_YOUR_KEY\"), \"Set your OPENAI_API_KEY environment variable or in the cell above!\"\n",
    "print(\"OpenAI client configured âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload Q&A_db_practice.json\")\n",
    "uploaded = files.upload()\n",
    "uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc53d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"Q&A_db_practice.json\")  # adjust if the uploaded file has a different name\n",
    "\n",
    "def load_qa(path: Path):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} Q&A pairs\")\n",
    "    print(\"Example keys:\", list(data[0].keys()))\n",
    "    return data\n",
    "\n",
    "qa_db = load_qa(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda7e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = qa_db[0]\n",
    "\n",
    "# Adapt the keys below if your JSON uses different names\n",
    "print(\"QUESTION:\\n\", example[\"question\"], \"\\n\")\n",
    "print(\"REFERENCE ANSWER (truncated):\\n\", textwrap.shorten(example[\"answer\"], width=300, placeholder=\"...\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff91c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Calls OpenAI chat completion and returns the text content.\n",
    "    Using a relatively small, cheap model for grading.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # change model if you prefer another\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a fair and rigorous ML exam grader.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d03bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_INSTRUCTIONS = \"\"\"\n",
    "You are grading a student's answer to a machine learning theory question.\n",
    "\n",
    "You will be given:\n",
    "- The question\n",
    "- A reference answer written by the instructor\n",
    "- The student's answer\n",
    "\n",
    "Tasks:\n",
    "1. Compare the student's answer with the reference.\n",
    "2. Comment on correctness, completeness, clarity, and missing points.\n",
    "3. Give an integer score from 0 to 100, where:\n",
    "   - 0â€“39: mostly incorrect or irrelevant\n",
    "   - 40â€“69: partially correct with important gaps\n",
    "   - 70â€“89: mostly correct with minor omissions\n",
    "   - 90â€“100: very complete and accurate.\n",
    "\n",
    "Respond ONLY in valid JSON with keys:\n",
    "- \"score\": integer 0â€“100\n",
    "- \"feedback\": short paragraph explanation.\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_answer(question: str, ref_answer: str, student_answer: str) -> dict:\n",
    "    prompt = f\"\"\"{EVAL_INSTRUCTIONS}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Reference answer:\n",
    "{ref_answer}\n",
    "\n",
    "Student answer:\n",
    "{student_answer}\n",
    "\n",
    "Now output the JSON:\"\"\"\n",
    "\n",
    "    raw = call_llm(prompt).strip()\n",
    "\n",
    "    # Handle ```json ... ``` wrappers if the model adds them\n",
    "    if raw.startswith(\"```\"):\n",
    "        raw = raw.strip(\"`\")\n",
    "        if raw.lower().startswith(\"json\"):\n",
    "            raw = raw[4:]\n",
    "        raw = raw.strip()\n",
    "\n",
    "    try:\n",
    "        result = json.loads(raw)\n",
    "    except Exception:\n",
    "        result = {\n",
    "            \"score\": 0,\n",
    "            \"feedback\": \"Could not parse JSON. Raw model output:\\n\" + raw\n",
    "        }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232cd9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.choice(qa_db)\n",
    "\n",
    "q = sample[\"question\"]\n",
    "ref = sample[\"answer\"]\n",
    "\n",
    "print(\"QUESTION:\\n\", q, \"\\n\")\n",
    "print(\"REFERENCE ANSWER (truncated):\\n\", textwrap.shorten(ref, width=350, placeholder=\"...\"), \"\\n\")\n",
    "\n",
    "# ðŸ‘‰ Replace this with your own attempt when testing\n",
    "student_answer = \"\"\"\n",
    "Here I would write my own explanation of the concept, using my own words.\n",
    "\"\"\"\n",
    "\n",
    "result = evaluate_answer(q, ref, student_answer)\n",
    "print(\"SCORE:\", result[\"score\"])\n",
    "print(\"\\nFEEDBACK:\\n\", result[\"feedback\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a9665",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def text_similarity(a: str, b: str) -> float:\n",
    "    \"\"\"Very simple lexical similarity as a baseline (0â€“100).\"\"\"\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio() * 100\n",
    "\n",
    "sim_score = text_similarity(student_answer, ref)\n",
    "print(f\"Naive similarity score (SequenceMatcher): {sim_score:.2f} / 100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95396d2",
   "metadata": {},
   "source": [
    "## Discussion of the approach\n",
    "\n",
    "In this part of the assignment, I implemented an LLM-based evaluator for open-ended\n",
    "ML theory questions. The pipeline is:\n",
    "\n",
    "1. Load a JSON file (`Q&A_db_practice.json`) with questions and reference answers.\n",
    "2. Let the student provide a free-text answer for a selected question.\n",
    "3. Send the question, reference answer and student answer to an OpenAI model\n",
    "   (`gpt-4o-mini`) with a carefully designed grading prompt.\n",
    "4. The model returns a JSON object with:\n",
    "   - a numerical score (0â€“100)\n",
    "   - an explanatory feedback paragraph.\n",
    "5. Optionally, I also compute a simple lexical similarity score between the\n",
    "   student and reference answers using `difflib.SequenceMatcher` as a baseline.\n",
    "\n",
    "This approach offloads the semantic comparison to the LLM, which can evaluate\n",
    "correctness and completeness beyond exact word overlap. In a real system, the\n",
    "LLM-based score and the lexical metric could be combined, and the collected\n",
    "scores and feedback could later be used for model fine-tuning or RLHF-style\n",
    "improvements.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
